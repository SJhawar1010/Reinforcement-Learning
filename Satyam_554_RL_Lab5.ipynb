{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJhawar1010/Reinforcement-Learning/blob/main/Satyam_554_RL_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f9466c-31fb-41ab-bd40-3b0f4ffe36ed",
      "metadata": {
        "id": "85f9466c-31fb-41ab-bd40-3b0f4ffe36ed"
      },
      "source": [
        "## Model-Free Prediction & Control With Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1b6c13-8568-486e-ba0e-2fffdd7bb59b",
      "metadata": {
        "id": "1d1b6c13-8568-486e-ba0e-2fffdd7bb59b"
      },
      "source": [
        "### Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "94df79a2-35cd-43ca-a963-8327dab51d96",
      "metadata": {
        "id": "94df79a2-35cd-43ca-a963-8327dab51d96"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "grid = np.array([\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 3, 0, 2],\n",
        "    [0, 0, 0, 0]\n",
        "])\n",
        "rewards = {0: -0.04, 1: 1, 2: -1, 3: 0}\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_effects = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "discount_factor = 0.9\n",
        "epsilon = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede54e9d-4a57-4d27-aad3-599d6daea875",
      "metadata": {
        "id": "ede54e9d-4a57-4d27-aad3-599d6daea875"
      },
      "source": [
        "### Check for terminal state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "68ff09a6-dfb7-43db-8413-854eac09ad60",
      "metadata": {
        "id": "68ff09a6-dfb7-43db-8413-854eac09ad60"
      },
      "outputs": [],
      "source": [
        "def is_terminal(state):\n",
        "    i, j = state\n",
        "    return grid[i, j] in (1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c311a5-3492-4f92-92d2-6e896862d245",
      "metadata": {
        "id": "98c311a5-3492-4f92-92d2-6e896862d245"
      },
      "source": [
        "### Next State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f7824bea-471b-46b1-aae0-200ac5347f66",
      "metadata": {
        "id": "f7824bea-471b-46b1-aae0-200ac5347f66"
      },
      "outputs": [],
      "source": [
        "def next(state, action):\n",
        "    i, j = state\n",
        "    di, dj = action_effects[action]\n",
        "    next_i, next_j = i + di, j + dj\n",
        "\n",
        "    if 0 <= next_i < grid.shape[0] and 0 <= next_j < grid.shape[1]:\n",
        "        return next_i, next_j\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ae7970-c355-46ad-b32c-15ca5eade7a6",
      "metadata": {
        "id": "43ae7970-c355-46ad-b32c-15ca5eade7a6"
      },
      "source": [
        "### Îµ-greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "68856bfe-e25b-4c7a-8a50-1ad5227c8700",
      "metadata": {
        "id": "68856bfe-e25b-4c7a-8a50-1ad5227c8700"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def epsilon_greedy_policy(Q, state):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        return max(actions, key=lambda a: Q[state, a])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb160962-62a5-4dc7-803e-ddb1c2f55965",
      "metadata": {
        "id": "eb160962-62a5-4dc7-803e-ddb1c2f55965"
      },
      "source": [
        "### Finding optimal Policy using using MC control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f44056e7-3989-4542-8a66-5f7b9000343c",
      "metadata": {
        "id": "f44056e7-3989-4542-8a66-5f7b9000343c"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def monte_carlo_control(num_episodes=5000):\n",
        "    Q = defaultdict(float)\n",
        "    returns = defaultdict(list)\n",
        "    policy = {}\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = (2, 0)\n",
        "        episode = []\n",
        "\n",
        "        while not is_terminal(state):\n",
        "            action = epsilon_greedy_policy(Q, state)\n",
        "            next_state = next(state, action)\n",
        "            reward = rewards[grid[state]]\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = reward + discount_factor * G\n",
        "            if (state, action) not in visited:\n",
        "                visited.add((state, action))\n",
        "                returns[(state, action)].append(G)\n",
        "                Q[state, action] = np.mean(returns[(state, action)])\n",
        "\n",
        "    for state_action in Q.keys():\n",
        "        state, action = state_action\n",
        "        if state not in policy:\n",
        "            policy[state] = max(actions, key=lambda a: Q[state, a])\n",
        "\n",
        "    return Q, policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6b08f9a2-85bb-4a0c-9b77-e37f06983a34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b08f9a2-85bb-4a0c-9b77-e37f06983a34",
        "outputId": "8495f756-1be3-4121-d008-ecfa31eb0003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "State (2, 0) Action up: -0.11\n",
            "State (2, 0) Action down: -0.15\n",
            "State (2, 0) Action left: -0.15\n",
            "State (2, 0) Action right: -0.12\n",
            "State (1, 0) Action up: -0.15\n",
            "State (1, 0) Action down: -0.15\n",
            "State (1, 0) Action left: -0.12\n",
            "State (1, 0) Action right: -0.08\n",
            "State (0, 0) Action up: -0.20\n",
            "State (0, 0) Action down: -0.20\n",
            "State (0, 0) Action left: -0.21\n",
            "State (0, 0) Action right: -0.12\n",
            "State (0, 1) Action up: -0.16\n",
            "State (0, 1) Action down: -0.08\n",
            "State (0, 1) Action left: -0.14\n",
            "State (0, 1) Action right: -0.13\n",
            "State (0, 2) Action up: -0.08\n",
            "State (0, 2) Action down: -0.08\n",
            "State (0, 2) Action left: -0.25\n",
            "State (0, 2) Action right: -0.04\n",
            "State (2, 1) Action up: -0.08\n",
            "State (2, 1) Action down: -0.13\n",
            "State (2, 1) Action left: -0.16\n",
            "State (2, 1) Action right: -0.16\n",
            "State (1, 1) Action up: -0.07\n",
            "State (1, 1) Action down: -0.08\n",
            "State (1, 1) Action left: -0.07\n",
            "State (1, 1) Action right: -0.04\n",
            "State (2, 2) Action up: -0.15\n",
            "State (2, 2) Action down: -0.17\n",
            "State (2, 2) Action left: -0.11\n",
            "State (2, 2) Action right: -0.12\n",
            "State (1, 2) Action up: -0.08\n",
            "State (1, 2) Action down: -0.14\n",
            "State (1, 2) Action left: -0.08\n",
            "State (1, 2) Action right: -0.04\n",
            "State (2, 3) Action up: -0.04\n",
            "State (2, 3) Action down: -0.08\n",
            "State (2, 3) Action left: -0.23\n",
            "State (2, 3) Action right: -0.40\n"
          ]
        }
      ],
      "source": [
        "Q, policy = monte_carlo_control()\n",
        "\n",
        "print(\"Learned Q-values:\")\n",
        "for key, value in Q.items():\n",
        "    print(f\"State {key[0]} Action {key[1]}: {value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c574ba0b-9682-4639-b179-202807e3f663",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c574ba0b-9682-4639-b179-202807e3f663",
        "outputId": "0f50019f-4755-4e7e-9762-5a2e4919329e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Derived Policy:\n",
            "State (2, 0): up\n",
            "State (1, 0): right\n",
            "State (0, 0): right\n",
            "State (0, 1): down\n",
            "State (0, 2): right\n",
            "State (2, 1): up\n",
            "State (1, 1): right\n",
            "State (2, 2): left\n",
            "State (1, 2): right\n",
            "State (2, 3): up\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDerived Policy:\")\n",
        "for state, action in policy.items():\n",
        "    print(f\"State {state}: {action}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fba385-3701-4940-beeb-fbdb4e0dd1f3",
      "metadata": {
        "id": "06fba385-3701-4940-beeb-fbdb4e0dd1f3"
      },
      "source": [
        "## END"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}